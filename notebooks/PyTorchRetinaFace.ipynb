{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models._utils as _utils\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def conv_bn(inp, oup, stride = 1, leaky = 0):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.LeakyReLU(negative_slope=leaky, inplace=True)\n",
    "    )\n",
    "\n",
    "def conv_bn_no_relu(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "    )\n",
    "\n",
    "def conv_bn1X1(inp, oup, stride, leaky=0):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, stride, padding=0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.LeakyReLU(negative_slope=leaky, inplace=True)\n",
    "    )\n",
    "\n",
    "def conv_dw(inp, oup, stride, leaky=0.1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n",
    "        nn.BatchNorm2d(inp),\n",
    "        nn.LeakyReLU(negative_slope= leaky,inplace=True),\n",
    "\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.LeakyReLU(negative_slope= leaky,inplace=True),\n",
    "    )\n",
    "\n",
    "class SSH(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super(SSH, self).__init__()\n",
    "        assert out_channel % 4 == 0\n",
    "        leaky = 0\n",
    "        if (out_channel <= 64):\n",
    "            leaky = 0.1\n",
    "        self.conv3X3 = conv_bn_no_relu(in_channel, out_channel//2, stride=1)\n",
    "\n",
    "        self.conv5X5_1 = conv_bn(in_channel, out_channel//4, stride=1, leaky = leaky)\n",
    "        self.conv5X5_2 = conv_bn_no_relu(out_channel//4, out_channel//4, stride=1)\n",
    "\n",
    "        self.conv7X7_2 = conv_bn(out_channel//4, out_channel//4, stride=1, leaky = leaky)\n",
    "        self.conv7x7_3 = conv_bn_no_relu(out_channel//4, out_channel//4, stride=1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        conv3X3 = self.conv3X3(input)\n",
    "\n",
    "        conv5X5_1 = self.conv5X5_1(input)\n",
    "        conv5X5 = self.conv5X5_2(conv5X5_1)\n",
    "\n",
    "        conv7X7_2 = self.conv7X7_2(conv5X5_1)\n",
    "        conv7X7 = self.conv7x7_3(conv7X7_2)\n",
    "\n",
    "        out = torch.cat([conv3X3, conv5X5, conv7X7], dim=1)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class FPN(nn.Module):\n",
    "    def __init__(self,in_channels_list,out_channels):\n",
    "        super(FPN,self).__init__()\n",
    "        leaky = 0\n",
    "        if (out_channels <= 64):\n",
    "            leaky = 0.1\n",
    "        self.output1 = conv_bn1X1(in_channels_list[0], out_channels, stride = 1, leaky = leaky)\n",
    "        self.output2 = conv_bn1X1(in_channels_list[1], out_channels, stride = 1, leaky = leaky)\n",
    "        self.output3 = conv_bn1X1(in_channels_list[2], out_channels, stride = 1, leaky = leaky)\n",
    "\n",
    "        self.merge1 = conv_bn(out_channels, out_channels, leaky = leaky)\n",
    "        self.merge2 = conv_bn(out_channels, out_channels, leaky = leaky)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # names = list(input.keys())\n",
    "        input = list(input.values())\n",
    "\n",
    "        output1 = self.output1(input[0])\n",
    "        output2 = self.output2(input[1])\n",
    "        output3 = self.output3(input[2])\n",
    "\n",
    "        up3 = F.interpolate(output3, size=[output2.size(2), output2.size(3)], mode=\"nearest\")\n",
    "        output2 = output2 + up3\n",
    "        output2 = self.merge2(output2)\n",
    "\n",
    "        up2 = F.interpolate(output2, size=[output1.size(2), output1.size(3)], mode=\"nearest\")\n",
    "        output1 = output1 + up2\n",
    "        output1 = self.merge1(output1)\n",
    "\n",
    "        out = [output1, output2, output3]\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class MobileNetV1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MobileNetV1, self).__init__()\n",
    "        self.stage1 = nn.Sequential(\n",
    "            conv_bn(3, 8, 2, leaky = 0.1),    # 3\n",
    "            conv_dw(8, 16, 1),   # 7\n",
    "            conv_dw(16, 32, 2),  # 11\n",
    "            conv_dw(32, 32, 1),  # 19\n",
    "            conv_dw(32, 64, 2),  # 27\n",
    "            conv_dw(64, 64, 1),  # 43\n",
    "        )\n",
    "        self.stage2 = nn.Sequential(\n",
    "            conv_dw(64, 128, 2),  # 43 + 16 = 59\n",
    "            conv_dw(128, 128, 1), # 59 + 32 = 91\n",
    "            conv_dw(128, 128, 1), # 91 + 32 = 123\n",
    "            conv_dw(128, 128, 1), # 123 + 32 = 155\n",
    "            conv_dw(128, 128, 1), # 155 + 32 = 187\n",
    "            conv_dw(128, 128, 1), # 187 + 32 = 219\n",
    "        )\n",
    "        self.stage3 = nn.Sequential(\n",
    "            conv_dw(128, 256, 2), # 219 +3 2 = 241\n",
    "            conv_dw(256, 256, 1), # 241 + 64 = 301\n",
    "        )\n",
    "        self.avg = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(256, 1000)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.avg(x)\n",
    "        # x = self.model(x)\n",
    "        x = x.view(-1, 256)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models.detection.backbone_utils as backbone_utils\n",
    "import torchvision.models._utils as _utils\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class ClassHead(nn.Module):\n",
    "    def __init__(self,inchannels=512,num_anchors=3):\n",
    "        super(ClassHead,self).__init__()\n",
    "        self.num_anchors = num_anchors\n",
    "        self.conv1x1 = nn.Conv2d(inchannels,self.num_anchors*2,kernel_size=(1,1),stride=1,padding=0)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.conv1x1(x)\n",
    "        out = out.permute(0,2,3,1).contiguous()\n",
    "        \n",
    "        return out.view(out.shape[0], -1, 2)\n",
    "\n",
    "class BboxHead(nn.Module):\n",
    "    def __init__(self,inchannels=512,num_anchors=3):\n",
    "        super(BboxHead,self).__init__()\n",
    "        self.conv1x1 = nn.Conv2d(inchannels,num_anchors*4,kernel_size=(1,1),stride=1,padding=0)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.conv1x1(x)\n",
    "        out = out.permute(0,2,3,1).contiguous()\n",
    "\n",
    "        return out.view(out.shape[0], -1, 4)\n",
    "\n",
    "class LandmarkHead(nn.Module):\n",
    "    def __init__(self,inchannels=512,num_anchors=3):\n",
    "        super(LandmarkHead,self).__init__()\n",
    "        self.conv1x1 = nn.Conv2d(inchannels,num_anchors*10,kernel_size=(1,1),stride=1,padding=0)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.conv1x1(x)\n",
    "        out = out.permute(0,2,3,1).contiguous()\n",
    "\n",
    "        return out.view(out.shape[0], -1, 10)\n",
    "\n",
    "class RetinaFace(nn.Module):\n",
    "    def __init__(self, cfg = None, phase = 'train'):\n",
    "        \"\"\"\n",
    "        :param cfg:  Network related settings.\n",
    "        :param phase: train or test.\n",
    "        \"\"\"\n",
    "        super(RetinaFace,self).__init__()\n",
    "        self.phase = phase\n",
    "        backbone = None\n",
    "        if cfg['name'] == 'mobilenet0.25':\n",
    "            backbone = MobileNetV1()\n",
    "            if cfg['pretrain']:\n",
    "                checkpoint = torch.load(\"./weights/mobilenetV1X0.25_pretrain.tar\", map_location=torch.device('cpu'))\n",
    "                from collections import OrderedDict\n",
    "                new_state_dict = OrderedDict()\n",
    "                for k, v in checkpoint['state_dict'].items():\n",
    "                    name = k[7:]  # remove module.\n",
    "                    new_state_dict[name] = v\n",
    "                # load params\n",
    "                backbone.load_state_dict(new_state_dict)\n",
    "        elif cfg['name'] == 'Resnet50':\n",
    "            import torchvision.models as models\n",
    "            backbone = models.resnet50(pretrained=cfg['pretrain'])\n",
    "\n",
    "        self.body = _utils.IntermediateLayerGetter(backbone, cfg['return_layers'])\n",
    "        in_channels_stage2 = cfg['in_channel']\n",
    "        in_channels_list = [\n",
    "            in_channels_stage2 * 2,\n",
    "            in_channels_stage2 * 4,\n",
    "            in_channels_stage2 * 8,\n",
    "        ]\n",
    "        out_channels = cfg['out_channel']\n",
    "        self.fpn = FPN(in_channels_list,out_channels)\n",
    "        self.ssh1 = SSH(out_channels, out_channels)\n",
    "        self.ssh2 = SSH(out_channels, out_channels)\n",
    "        self.ssh3 = SSH(out_channels, out_channels)\n",
    "\n",
    "        self.ClassHead = self._make_class_head(fpn_num=3, inchannels=cfg['out_channel'])\n",
    "        self.BboxHead = self._make_bbox_head(fpn_num=3, inchannels=cfg['out_channel'])\n",
    "        self.LandmarkHead = self._make_landmark_head(fpn_num=3, inchannels=cfg['out_channel'])\n",
    "\n",
    "    def _make_class_head(self,fpn_num=3,inchannels=64,anchor_num=2):\n",
    "        classhead = nn.ModuleList()\n",
    "        for i in range(fpn_num):\n",
    "            classhead.append(ClassHead(inchannels,anchor_num))\n",
    "        return classhead\n",
    "    \n",
    "    def _make_bbox_head(self,fpn_num=3,inchannels=64,anchor_num=2):\n",
    "        bboxhead = nn.ModuleList()\n",
    "        for i in range(fpn_num):\n",
    "            bboxhead.append(BboxHead(inchannels,anchor_num))\n",
    "        return bboxhead\n",
    "\n",
    "    def _make_landmark_head(self,fpn_num=3,inchannels=64,anchor_num=2):\n",
    "        landmarkhead = nn.ModuleList()\n",
    "        for i in range(fpn_num):\n",
    "            landmarkhead.append(LandmarkHead(inchannels,anchor_num))\n",
    "        return landmarkhead\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        out = self.body(inputs)\n",
    "\n",
    "        # FPN\n",
    "        fpn = self.fpn(out)\n",
    "\n",
    "        # SSH\n",
    "        feature1 = self.ssh1(fpn[0])\n",
    "        feature2 = self.ssh2(fpn[1])\n",
    "        feature3 = self.ssh3(fpn[2])\n",
    "        features = [feature1, feature2, feature3]\n",
    "\n",
    "        bbox_regressions = torch.cat([self.BboxHead[i](feature) for i, feature in enumerate(features)], dim=1)\n",
    "        classifications = torch.cat([self.ClassHead[i](feature) for i, feature in enumerate(features)],dim=1)\n",
    "        ldm_regressions = torch.cat([self.LandmarkHead[i](feature) for i, feature in enumerate(features)], dim=1)\n",
    "\n",
    "        if self.phase == 'train':\n",
    "            output = (bbox_regressions, classifications, ldm_regressions)\n",
    "        else:\n",
    "            output = (bbox_regressions, F.softmax(classifications, dim=-1), ldm_regressions)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from itertools import product as product\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "\n",
    "\n",
    "class PriorBox(object):\n",
    "    def __init__(self, cfg, image_size=None, phase='train'):\n",
    "        super(PriorBox, self).__init__()\n",
    "        self.min_sizes = cfg['min_sizes']\n",
    "        self.steps = cfg['steps']\n",
    "        self.clip = cfg['clip']\n",
    "        self.image_size = image_size\n",
    "        self.feature_maps = [[ceil(self.image_size[0]/step), ceil(self.image_size[1]/step)] for step in self.steps]\n",
    "        self.name = \"s\"\n",
    "\n",
    "    def forward(self):\n",
    "        anchors = []\n",
    "        for k, f in enumerate(self.feature_maps):\n",
    "            min_sizes = self.min_sizes[k]\n",
    "            for i, j in product(range(f[0]), range(f[1])):\n",
    "                for min_size in min_sizes:\n",
    "                    s_kx = min_size / self.image_size[1]\n",
    "                    s_ky = min_size / self.image_size[0]\n",
    "                    dense_cx = [x * self.steps[k] / self.image_size[1] for x in [j + 0.5]]\n",
    "                    dense_cy = [y * self.steps[k] / self.image_size[0] for y in [i + 0.5]]\n",
    "                    for cy, cx in product(dense_cy, dense_cx):\n",
    "                        anchors += [cx, cy, s_kx, s_ky]\n",
    "\n",
    "        # back to torch land\n",
    "        output = torch.Tensor(anchors).view(-1, 4)\n",
    "        if self.clip:\n",
    "            output.clamp_(max=1, min=0)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/Hakuyume/chainer-ssd\n",
    "def decode(loc, priors, variances):\n",
    "    \"\"\"Decode locations from predictions using priors to undo\n",
    "    the encoding we did for offset regression at train time.\n",
    "    Args:\n",
    "        loc (tensor): location predictions for loc layers,\n",
    "            Shape: [num_priors,4]\n",
    "        priors (tensor): Prior boxes in center-offset form.\n",
    "            Shape: [num_priors,4].\n",
    "        variances: (list[float]) Variances of priorboxes\n",
    "    Return:\n",
    "        decoded bounding box predictions\n",
    "    \"\"\"\n",
    "\n",
    "    boxes = torch.cat((\n",
    "        priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n",
    "        priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1)\n",
    "    boxes[:, :2] -= boxes[:, 2:] / 2\n",
    "    boxes[:, 2:] += boxes[:, :2]\n",
    "    return boxes\n",
    "\n",
    "def decode_landm(pre, priors, variances):\n",
    "    \"\"\"Decode landm from predictions using priors to undo\n",
    "    the encoding we did for offset regression at train time.\n",
    "    Args:\n",
    "        pre (tensor): landm predictions for loc layers,\n",
    "            Shape: [num_priors,10]\n",
    "        priors (tensor): Prior boxes in center-offset form.\n",
    "            Shape: [num_priors,4].\n",
    "        variances: (list[float]) Variances of priorboxes\n",
    "    Return:\n",
    "        decoded landm predictions\n",
    "    \"\"\"\n",
    "    landms = torch.cat((priors[:, :2] + pre[:, :2] * variances[0] * priors[:, 2:],\n",
    "                        priors[:, :2] + pre[:, 2:4] * variances[0] * priors[:, 2:],\n",
    "                        priors[:, :2] + pre[:, 4:6] * variances[0] * priors[:, 2:],\n",
    "                        priors[:, :2] + pre[:, 6:8] * variances[0] * priors[:, 2:],\n",
    "                        priors[:, :2] + pre[:, 8:10] * variances[0] * priors[:, 2:],\n",
    "                        ), dim=1)\n",
    "    return landms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------\n",
    "# Fast R-CNN\n",
    "# Copyright (c) 2015 Microsoft\n",
    "# Licensed under The MIT License [see LICENSE for details]\n",
    "# Written by Ross Girshick\n",
    "# --------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def py_cpu_nms(dets, thresh):\n",
    "    \"\"\"Pure Python NMS baseline.\"\"\"\n",
    "    x1 = dets[:, 0]\n",
    "    y1 = dets[:, 1]\n",
    "    x2 = dets[:, 2]\n",
    "    y2 = dets[:, 3]\n",
    "    scores = dets[:, 4]\n",
    "\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    order = scores.argsort()[::-1]\n",
    "\n",
    "    keep = []\n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "        inter = w * h\n",
    "        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "\n",
    "        inds = np.where(ovr <= thresh)[0]\n",
    "        order = order[inds + 1]\n",
    "\n",
    "    return keep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "\n",
    "cfg_re50 = {\n",
    "    'name': 'Resnet50',\n",
    "    'min_sizes': [[16, 32], [64, 128], [256, 512]],\n",
    "    'steps': [8, 16, 32],\n",
    "    'variance': [0.1, 0.2],\n",
    "    'clip': False,\n",
    "    'loc_weight': 2.0,\n",
    "    'gpu_train': True,\n",
    "    'batch_size': 24,\n",
    "    'ngpu': 4,\n",
    "    'epoch': 100,\n",
    "    'decay1': 70,\n",
    "    'decay2': 90,\n",
    "    'image_size': 840,\n",
    "    'pretrain': True,\n",
    "    'return_layers': {'layer2': 1, 'layer3': 2, 'layer4': 3},\n",
    "    'in_channel': 256,\n",
    "    'out_channel': 256\n",
    "}\n",
    "\n",
    "\n",
    "def check_keys(model, pretrained_state_dict):\n",
    "    ckpt_keys = set(pretrained_state_dict.keys())\n",
    "    model_keys = set(model.state_dict().keys())\n",
    "    used_pretrained_keys = model_keys & ckpt_keys\n",
    "    unused_pretrained_keys = ckpt_keys - model_keys\n",
    "    missing_keys = model_keys - ckpt_keys\n",
    "    print('Missing keys:{}'.format(len(missing_keys)))\n",
    "    print('Unused checkpoint keys:{}'.format(len(unused_pretrained_keys)))\n",
    "    print('Used keys:{}'.format(len(used_pretrained_keys)))\n",
    "    assert len(used_pretrained_keys) > 0, 'load NONE from pretrained checkpoint'\n",
    "    return True\n",
    "\n",
    "\n",
    "def remove_prefix(state_dict, prefix):\n",
    "    ''' Old style model is stored with all names of parameters sharing common prefix 'module.' '''\n",
    "    print('remove prefix \\'{}\\''.format(prefix))\n",
    "    f = lambda x: x.split(prefix, 1)[-1] if x.startswith(prefix) else x\n",
    "    return {f(key): value for key, value in state_dict.items()}\n",
    "\n",
    "\n",
    "def load_model(model, pretrained_path, load_to_cpu):\n",
    "    print('Loading pretrained model from {}'.format(pretrained_path))\n",
    "    if load_to_cpu:\n",
    "        pretrained_dict = torch.load(pretrained_path, map_location=lambda storage, loc: storage)\n",
    "    else:\n",
    "        device = torch.cuda.current_device()\n",
    "        pretrained_dict = torch.load(pretrained_path, map_location=lambda storage, loc: storage.cuda(device))\n",
    "    if \"state_dict\" in pretrained_dict.keys():\n",
    "        pretrained_dict = remove_prefix(pretrained_dict['state_dict'], 'module.')\n",
    "    else:\n",
    "        pretrained_dict = remove_prefix(pretrained_dict, 'module.')\n",
    "    check_keys(model, pretrained_dict)\n",
    "    model.load_state_dict(pretrained_dict, strict=False)\n",
    "    return model\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     torch.set_grad_enabled(False)\n",
    "#     cfg = None\n",
    "#     if args.network == \"mobile0.25\":\n",
    "#         cfg = cfg_mnet\n",
    "#     elif args.network == \"resnet50\":\n",
    "#         cfg = cfg_re50\n",
    "#     # net and model\n",
    "#     net = RetinaFace(cfg=cfg, phase = 'test')\n",
    "#     net = load_model(net, args.trained_model, args.cpu)\n",
    "#     net.eval()\n",
    "#     print('Finished loading model!')\n",
    "#     print(net)\n",
    "#     cudnn.benchmark = True\n",
    "#     device = torch.device(\"cpu\" if args.cpu else \"cuda\")\n",
    "#     net = net.to(device)\n",
    "\n",
    "#     resize = 1\n",
    "\n",
    "#     # testing begin\n",
    "#     for i in range(100):\n",
    "#         image_path = \"./curve/test.jpg\"\n",
    "#         img_raw = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "\n",
    "#         img = np.float32(img_raw)\n",
    "\n",
    "#         im_height, im_width, _ = img.shape\n",
    "#         scale = torch.Tensor([img.shape[1], img.shape[0], img.shape[1], img.shape[0]])\n",
    "#         img -= (104, 117, 123)\n",
    "#         img = img.transpose(2, 0, 1)\n",
    "#         img = torch.from_numpy(img).unsqueeze(0)\n",
    "#         img = img.to(device)\n",
    "#         scale = scale.to(device)\n",
    "\n",
    "#         tic = time.time()\n",
    "#         loc, conf, landms = net(img)  # forward pass\n",
    "#         print('net forward time: {:.4f}'.format(time.time() - tic))\n",
    "\n",
    "#         priorbox = PriorBox(cfg, image_size=(im_height, im_width))\n",
    "#         priors = priorbox.forward()\n",
    "#         priors = priors.to(device)\n",
    "#         prior_data = priors.data\n",
    "#         boxes = decode(loc.data.squeeze(0), prior_data, cfg['variance'])\n",
    "#         boxes = boxes * scale / resize\n",
    "#         boxes = boxes.cpu().numpy()\n",
    "#         scores = conf.squeeze(0).data.cpu().numpy()[:, 1]\n",
    "#         landms = decode_landm(landms.data.squeeze(0), prior_data, cfg['variance'])\n",
    "#         scale1 = torch.Tensor([img.shape[3], img.shape[2], img.shape[3], img.shape[2],\n",
    "#                                img.shape[3], img.shape[2], img.shape[3], img.shape[2],\n",
    "#                                img.shape[3], img.shape[2]])\n",
    "#         scale1 = scale1.to(device)\n",
    "#         landms = landms * scale1 / resize\n",
    "#         landms = landms.cpu().numpy()\n",
    "\n",
    "#         # ignore low scores\n",
    "#         inds = np.where(scores > args.confidence_threshold)[0]\n",
    "#         boxes = boxes[inds]\n",
    "#         landms = landms[inds]\n",
    "#         scores = scores[inds]\n",
    "\n",
    "#         # keep top-K before NMS\n",
    "#         order = scores.argsort()[::-1][:args.top_k]\n",
    "#         boxes = boxes[order]\n",
    "#         landms = landms[order]\n",
    "#         scores = scores[order]\n",
    "\n",
    "#         # do NMS\n",
    "#         dets = np.hstack((boxes, scores[:, np.newaxis])).astype(np.float32, copy=False)\n",
    "#         keep = py_cpu_nms(dets, args.nms_threshold)\n",
    "#         # keep = nms(dets, args.nms_threshold,force_cpu=args.cpu)\n",
    "#         dets = dets[keep, :]\n",
    "#         landms = landms[keep]\n",
    "\n",
    "#         # keep top-K faster NMS\n",
    "#         dets = dets[:args.keep_top_k, :]\n",
    "#         landms = landms[:args.keep_top_k, :]\n",
    "\n",
    "#         dets = np.concatenate((dets, landms), axis=1)\n",
    "\n",
    "#         # show image\n",
    "#         if args.save_image:\n",
    "#             for b in dets:\n",
    "#                 if b[4] < args.vis_thres:\n",
    "#                     continue\n",
    "#                 text = \"{:.4f}\".format(b[4])\n",
    "#                 b = list(map(int, b))\n",
    "#                 cv2.rectangle(img_raw, (b[0], b[1]), (b[2], b[3]), (0, 0, 255), 2)\n",
    "#                 cx = b[0]\n",
    "#                 cy = b[1] + 12\n",
    "#                 cv2.putText(img_raw, text, (cx, cy),\n",
    "#                             cv2.FONT_HERSHEY_DUPLEX, 0.5, (255, 255, 255))\n",
    "\n",
    "#                 # landms\n",
    "#                 cv2.circle(img_raw, (b[5], b[6]), 1, (0, 0, 255), 4)\n",
    "#                 cv2.circle(img_raw, (b[7], b[8]), 1, (0, 255, 255), 4)\n",
    "#                 cv2.circle(img_raw, (b[9], b[10]), 1, (255, 0, 255), 4)\n",
    "#                 cv2.circle(img_raw, (b[11], b[12]), 1, (0, 255, 0), 4)\n",
    "#                 cv2.circle(img_raw, (b[13], b[14]), 1, (255, 0, 0), 4)\n",
    "#             # save image\n",
    "\n",
    "#             name = \"test.jpg\"\n",
    "#             cv2.imwrite(name, img_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models.detection.backbone_utils as backbone_utils\n",
    "import torchvision.models._utils as _utils\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class RetinaFace(nn.Module):\n",
    "    \n",
    "    def __init__(self, cfg = None, phase = 'train'):\n",
    "        \"\"\"\n",
    "        :param cfg:  Network related settings.\n",
    "        :param phase: train or test.\n",
    "        \"\"\"\n",
    "        super(RetinaFace, self).__init__()\n",
    "        self.phase = phase\n",
    "        backbone = None\n",
    "        if cfg['name'] == 'mobilenet0.25':\n",
    "            backbone = MobileNetV1()\n",
    "            if cfg['pretrain']:\n",
    "                checkpoint = torch.load(\"./weights/mobilenetV1X0.25_pretrain.tar\", map_location=torch.device('cpu'))\n",
    "                from collections import OrderedDict\n",
    "                new_state_dict = OrderedDict()\n",
    "                for k, v in checkpoint['state_dict'].items():\n",
    "                    name = k[7:]  # remove module.\n",
    "                    new_state_dict[name] = v\n",
    "                # load params\n",
    "                backbone.load_state_dict(new_state_dict)\n",
    "        elif cfg['name'] == 'Resnet50':\n",
    "            import torchvision.models as models\n",
    "            backbone = models.resnet50(pretrained=cfg['pretrain'])\n",
    "\n",
    "        self.body = _utils.IntermediateLayerGetter(backbone, cfg['return_layers'])\n",
    "        in_channels_stage2 = cfg['in_channel']\n",
    "        in_channels_list = [\n",
    "            in_channels_stage2 * 2,\n",
    "            in_channels_stage2 * 4,\n",
    "            in_channels_stage2 * 8,\n",
    "        ]\n",
    "        out_channels = cfg['out_channel']\n",
    "        self.fpn = FPN(in_channels_list,out_channels)\n",
    "        self.ssh1 = SSH(out_channels, out_channels)\n",
    "        self.ssh2 = SSH(out_channels, out_channels)\n",
    "        self.ssh3 = SSH(out_channels, out_channels)\n",
    "\n",
    "        self.ClassHead = self._make_class_head(fpn_num=3, inchannels=cfg['out_channel'])\n",
    "        self.BboxHead = self._make_bbox_head(fpn_num=3, inchannels=cfg['out_channel'])\n",
    "        self.LandmarkHead = self._make_landmark_head(fpn_num=3, inchannels=cfg['out_channel'])\n",
    "\n",
    "    def _make_class_head(self,fpn_num=3,inchannels=64,anchor_num=2):\n",
    "        classhead = nn.ModuleList()\n",
    "        for i in range(fpn_num):\n",
    "            classhead.append(ClassHead(inchannels,anchor_num))\n",
    "        return classhead\n",
    "    \n",
    "    def _make_bbox_head(self,fpn_num=3,inchannels=64,anchor_num=2):\n",
    "        bboxhead = nn.ModuleList()\n",
    "        for i in range(fpn_num):\n",
    "            bboxhead.append(BboxHead(inchannels,anchor_num))\n",
    "        return bboxhead\n",
    "\n",
    "    def _make_landmark_head(self,fpn_num=3,inchannels=64,anchor_num=2):\n",
    "        landmarkhead = nn.ModuleList()\n",
    "        for i in range(fpn_num):\n",
    "            landmarkhead.append(LandmarkHead(inchannels,anchor_num))\n",
    "        return landmarkhead\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        out = self.body(inputs)\n",
    "\n",
    "        # FPN\n",
    "        fpn = self.fpn(out)\n",
    "\n",
    "        # SSH\n",
    "        feature1 = self.ssh1(fpn[0])\n",
    "        feature2 = self.ssh2(fpn[1])\n",
    "        feature3 = self.ssh3(fpn[2])\n",
    "        features = [feature1, feature2, feature3]\n",
    "\n",
    "        bbox_regressions = torch.cat([self.BboxHead[i](feature) for i, feature in enumerate(features)], dim=1)\n",
    "        classifications = torch.cat([self.ClassHead[i](feature) for i, feature in enumerate(features)],dim=1)\n",
    "        ldm_regressions = torch.cat([self.LandmarkHead[i](feature) for i, feature in enumerate(features)], dim=1)\n",
    "        return (bbox_regressions, F.softmax(classifications, dim=-1), ldm_regressions)\n",
    "\n",
    "class ClassHead(nn.Module):\n",
    "    def __init__(self,inchannels=512,num_anchors=3):\n",
    "        super(ClassHead,self).__init__()\n",
    "        self.num_anchors = num_anchors\n",
    "        self.conv1x1 = nn.Conv2d(inchannels,self.num_anchors*2,kernel_size=(1,1),stride=1,padding=0)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.conv1x1(x)\n",
    "        out = out.permute(0,2,3,1).contiguous()\n",
    "        \n",
    "        return out.view(out.shape[0], -1, 2)\n",
    "\n",
    "class BboxHead(nn.Module):\n",
    "    def __init__(self,inchannels=512,num_anchors=3):\n",
    "        super(BboxHead,self).__init__()\n",
    "        self.conv1x1 = nn.Conv2d(inchannels,num_anchors*4,kernel_size=(1,1),stride=1,padding=0)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.conv1x1(x)\n",
    "        out = out.permute(0,2,3,1).contiguous()\n",
    "\n",
    "        return out.view(out.shape[0], -1, 4)\n",
    "\n",
    "class LandmarkHead(nn.Module):\n",
    "    def __init__(self,inchannels=512,num_anchors=3):\n",
    "        super(LandmarkHead,self).__init__()\n",
    "        self.conv1x1 = nn.Conv2d(inchannels,num_anchors*10,kernel_size=(1,1),stride=1,padding=0)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.conv1x1(x)\n",
    "        out = out.permute(0,2,3,1).contiguous()\n",
    "\n",
    "        return out.view(out.shape[0], -1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1, 2, 3], [4, 5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "[4 5 6]\n"
     ]
    }
   ],
   "source": [
    "for i in a:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_tools",
   "language": "python",
   "name": "cv_tools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
